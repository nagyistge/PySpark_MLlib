{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression model build with cross validation and evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import basic pyspark libraries\n",
    "import pyspark\n",
    "from pyspark import SQLContext\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import basic python libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import model build and evaluation libraries\n",
    "from pyspark.ml import Pipeline \n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import plotting libraries\n",
    "import plotly\n",
    "from plotly.offline import plot\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset from S3 and rename dependent variable column as label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load dataset from S3 and rename dependent variable column \n",
    "filepath = '/somefilepathhere/'\n",
    "df = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferschema\", \"true\").option(\"mode\", \"DROPMALFORMED\").load(filepath)\n",
    "df = df.withColumnRenamed(\"flagged\", \"label\")\n",
    "df.cache()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set training, testing ratio and split seed number\n",
    "train_ratio = 0.9\n",
    "test_ratio = 1 - train_ratio\n",
    "split_seed = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split dataframe\n",
    "df_train, df_test = df.randomSplit([train_ratio,test_ratio], seed = split_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build model: logistic regression with cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Set up feature transformation and logistic regression in pipeline (estimator)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up the assembler to create the features vector for logistic regression\n",
    "assembler = (VectorAssembler(inputCols=[x for x in df_train.columns if x not in ['label']], outputCol='features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up binomial logistic regression model with elastic net regularization.\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use pipeline to configure ML (first assemble features, then fit logistic regression)\n",
    "pipeline = Pipeline(stages = [assembler, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Set up logistic regression parameter grid for cross validation (estimator parameter map)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# maxIter indicates the maximum number of iteration for each model fitting\n",
    "param_maxIter = [10,15,20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# regParam is lambda (>0) and elasticNetParam is alpha (0 <= alpha <= 1)\n",
    "# alpha and lambda indicates L1 and L2 regularization\n",
    "# when alpha = 1 --> lasso; when alpha = 0 --> ridge\n",
    "# regularization prevents overfitting \n",
    "param_reg = [0.0, 0.05, 0.1]\n",
    "param_elasticNet = [0.8, 0.85, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up the parameter grid\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.maxIter, [10,15]).addGrid(lr.regParam, [0.0, 0.1]).addGrid(lr.elasticNetParam, [0.9, 0.8]).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Set up model evaluator for cross validation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up model evaluator\n",
    "evaluator = BinaryClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use k-fold cross validation to fit the dataset and find the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set number of folds for cross validation\n",
    "k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up cross validation and fit the training dataset\n",
    "crossval = CrossValidator(estimator = pipeline, estimatorParamMaps = paramGrid, evaluator = evaluator, numFolds = k)\n",
    "cvModel = crossval.fit(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model result and generate output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Evaluate model result*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the sample size\n",
    "training_sample_size = df_train.count()\n",
    "test_sample_size = df_test.count()\n",
    "total_sample_size = df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get average matrics for each configuration in cross validation, and the index of best model\n",
    "avgMetrics = pd.DataFrame(cvModel.avgMetrics, columns = ['avgMetric'])\n",
    "best_index = avgMetrics[avgMetrics['avgMetric'] == avgMetrics.max().iloc[0]].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find best model and summary\n",
    "best = cvModel.bestModel.stages[1]\n",
    "bestSum = best.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the parameters of the best model\n",
    "param = pd.Series(cvModel.getEstimatorParamMaps()[best_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get coefficients and odds ratio of each feature\n",
    "coef = pd.DataFrame(data = best.coefficients.values,index = [x for x in df_train.columns if x not in ['label']], columns = ['Coefficient'])\n",
    "coef['Odds_ratio'] = coef['Coefficient'].apply(np.exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get intercept\n",
    "intercept = best.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get number of classes(labels) and number of features\n",
    "num_classes = best.numClasses\n",
    "num_features = best.numFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get full table of label, features, probability, and prediction\n",
    "predictions = bestSum.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get area under ROC\n",
    "AUROC = bestSum.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert prediciton and label column in predictions dataframe to RDD and instantiate metrics object\n",
    "prediction_label = predictions.select('prediction', 'label').rdd\n",
    "metrics = MulticlassMetrics(prediction_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get best model's F-1 score\n",
    "f1_score = metrics.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get best model's precision of true and false\n",
    "# precision of true\n",
    "precision_T = metrics.precision(1)\n",
    "# precision of false\n",
    "precision_F = metrics.precision(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get best model's recall of true and false\n",
    "# recall of true\n",
    "recall_T = metrics.recall(1)\n",
    "# recall of false\n",
    "recall_F = metrics.recall(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get best model's confusion matrix\n",
    "confusion_mtrx = metrics.confusionMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot ROC curve\n",
    "roc = bestSum.roc.toPandas()\n",
    "lw = 2\n",
    "\n",
    "trace_roc1 = go.Scatter(x=roc['FPR'], y=roc['TPR'], mode='lines', line=dict(color='darkorange', width=lw),name='ROC curve (area = %0.2f)' %bestSum.areaUnderROC)\n",
    "\n",
    "trace_roc2 = go.Scatter(x=[0, 1], y=[0, 1], mode='lines', line=dict(color='navy', width=lw, dash='dash'),showlegend=False)\n",
    "\n",
    "layout_roc = go.Layout(title='Receiver operating characteristic',xaxis=dict(title='False Positive Rate'),yaxis=dict(title='True Positive Rate'))\n",
    "\n",
    "fig_roc = plot(go.Figure(data=[trace_roc1, trace_roc2], layout=layout_roc), output_type='div')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot precision against recall value\n",
    "precision_recall = bestSum.pr.toPandas()\n",
    "\n",
    "trace_pr = go.Scatter(x=precision_recall['recall'], y=precision_recall['precision'], mode='lines', line=dict(color='green', width=lw), name='precision recall curve')\n",
    "\n",
    "layout_pr = go.Layout(title='Precision Recall',xaxis=dict(title='Recall'),yaxis=dict(title='Precision'))\n",
    "\n",
    "fig_pr = plot(go.Figure(data=[trace_pr], layout=layout_pr), output_type='div')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot F Measure, precision, and recall by threshold\n",
    "fMeasure = bestSum.fMeasureByThreshold.toPandas()\n",
    "precision_thre = bestSum.precisionByThreshold.toPandas()\n",
    "recall_thre = bestSum.recallByThreshold.toPandas()\n",
    "\n",
    "trace_fM_thre = go.Scatter(x=fMeasure['threshold'], y=fMeasure['F-Measure'], mode='lines', line=dict(color='green', width=lw), name='F Measure By Threshold Curve')\n",
    "\n",
    "trace_pre_thre = go.Scatter(x=precision_thre['threshold'], y=precision_recall['precision'], mode='lines', line=dict(color='purple', width=lw), name='Precision By Threshold Curve')\n",
    "\n",
    "trace_rec_thre = go.Scatter(x=recall_thre['threshold'], y=recall_thre['recall'], mode='lines', line=dict(color='orange', width=lw), name='Recall By Threshold Curve')\n",
    "\n",
    "layout_fpr_thre = go.Layout(title='F-Measure, Precision, and Recall by Threshold',xaxis=dict(title='Threshold'),yaxis=dict(title='Value'))\n",
    "\n",
    "fig_fpr_thre = plot(go.Figure(data=[trace_fM_thre, trace_pre_thre, trace_rec_thre], layout=layout_fpr_thre), output_type='div')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output model evaluation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function to print all results\n",
    "def print_result():\n",
    "  print('\\n Cross Validation Evaludation: ')\n",
    "  print('\\n Total Sample Size: %i' %total_sample_size)\n",
    "  print('\\n Training Sample Size: %i' %training_sample_size)\n",
    "  print('\\n Test Sample Size: %i' %test_sample_size)\n",
    "  print('\\n Average Metrics for Cross Validation: ')\n",
    "  print(avgMetrics)\n",
    "  print('\\n ****************************************************')\n",
    "  print('\\n Best Logistic Regression Model Evaluation: ')\n",
    "  print('\\n Parameters of Best Model: ')\n",
    "  print(param)\n",
    "  print('\\n Number of Label Classes: %i' %num_classes)\n",
    "  print('\\n Number of Features: %i' %num_features)\n",
    "  print('\\n Coefficients and Odds Ratio: ')\n",
    "  print(coef)\n",
    "  print('\\n Intercept: %0.18f' %intercept)\n",
    "  print('\\n Area Under ROC: %0.18f' %AUROC)\n",
    "  print('\\n F-1 Score: %0.18f' %f1_score)\n",
    "  print('\\n Precision of True: %0.18f' %precision_T)\n",
    "  print('\\n Precision of False: %0.18f' %precision_F)\n",
    "  print('\\n Recall of True: %0.18f' %recall_T)\n",
    "  print('\\n Recall of False: %0.18f' %recall_F)\n",
    "  print('\\n Confusion Matrix: ')\n",
    "  print(confusion_mtrx.toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print evaluation result\n",
    "print_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display ROC curve\n",
    "displayHTML(fig_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display precision_recall curve\n",
    "displayHTML(fig_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display F Measure, precision, and recall by threshold graph\n",
    "displayHTML(fig_fpr_thre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "name": "model_build_and_eval",
  "notebookId": 598789
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
